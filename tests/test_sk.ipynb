{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew, ks_2samp # Kolmogorov-Smirnov Test\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, mean_absolute_error, r2_score \\\n",
    "                            , precision_recall_fscore_support, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch data\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "# fetch dataset \n",
    "uci_data = fetch_ucirepo(id=544) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = uci_data.data.features \n",
    "y = uci_data.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "X = X.apply(lambda x: le.fit_transform(x) if x.dtype == 'object' else x)\n",
    "y = y.apply(lambda x: le.fit_transform(x) if x.dtype == 'object' else x)\n",
    "\n",
    "\n",
    "# Drop all columns with NA\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "# Flatten the y\n",
    "y = y.values.ravel()\n",
    "\n",
    "\n",
    "## Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:\n",
      " 0.943217665615142\n"
     ]
    }
   ],
   "source": [
    "## Try Random Forest\n",
    "random_forest_pipeline = RandomForestClassifier(random_state=random_seed, max_depth=10)\n",
    "random_forest_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classification Accuracy:\\n\", random_forest_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8359621451104101"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = random_forest_pipeline.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_base_on_metric(y_pred, y_true, metric):\n",
    "    supported = [[\"MSE\", \"MAE\", \"r2\"],[\"accuracy\", \"precision\", \"recall\", \"f1\"], [\"log_loss\"]]\n",
    "    if metric == \"MSE\":\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    elif metric == \"MAE\":\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    elif metric == \"r2\":\n",
    "        return r2_score(y_true, y_pred)\n",
    "    elif metric == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif metric == \"precision\":\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        return precision\n",
    "    elif metric == \"recall\":\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        return recall\n",
    "    elif metric == \"f1\":\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        return f1\n",
    "    elif metric == \"log_loss\":\n",
    "        return log_loss(y_true, y_pred)\n",
    "    else:\n",
    "        raise KeyError(\"Unsupported Metric\")\n",
    "    \n",
    "def calculate_influence_base_on_metric(base_score, current_score, metric):\n",
    "    # Errors: lower better\n",
    "    if metric in [\"MSE\", \"MAE\", \"log_loss\"]:\n",
    "        influence = current_score - base_score\n",
    "    # Accuracy: higher better\n",
    "    elif metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"r2\"]:\n",
    "        influence = base_score - current_score\n",
    "    else:\n",
    "        raise KeyError(\"Unsupported Metric\")\n",
    "    return influence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfluenceAnalyze():\n",
    "    def __init__(self, model, X, y, task, metric=None):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        assert X.shape[0] == y.shape[0], \"Unmatched X, y size\"\n",
    "\n",
    "        self.data_category = \"tabular\"\n",
    "\n",
    "        self.data_influences = np.zeros(len(X))\n",
    "        self.feature_influences = None\n",
    "        self.influence_method = None\n",
    "\n",
    "        self.supported_tasks = [\"regression\", \"classification\", \"probabilities\"]\n",
    "        self.task = task\n",
    "        if task not in self.supported_tasks:\n",
    "            print(\"Supported tasks:\", self.supported_tasks)\n",
    "            raise KeyError(\"Unsupported Task\")\n",
    "        \n",
    "        self.supported_metrics = [[\"MSE\", \"MAE\", \"r2\"],[\"accuracy\", \"precision\", \"recall\", \"f1\"], [\"log_loss\"]]\n",
    "        self.metric = None\n",
    "\n",
    "        if metric:\n",
    "            if metric not in self.supported_metrics[self.supported_tasks.index(task)]:\n",
    "                print(f\"Supported metrics for {task}:\", self.supported_metrics[self.supported_tasks.index(task)])\n",
    "                raise KeyError(\"Unsupported Metric for this task\")\n",
    "            else:\n",
    "                self.metric = metric\n",
    "        else:\n",
    "            # Default chose the first metric\n",
    "            self.metric = self.supported_metrics[self.supported_tasks.index(task)][0]\n",
    "\n",
    "        self.preprocess_pipeline = None\n",
    "\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(self.X)\n",
    "        self.base_score = calculate_score_base_on_metric(y_pred, y, self.metric)\n",
    "\n",
    "        print(f\"Data size, X: {X.shape}, y: {y.shape}\")\n",
    "        print(f\"Task: {self.task}, using metric: {self.metric}\")\n",
    "        print(f\"Base score: {self.base_score}\")\n",
    "    \n",
    "    def help(self):\n",
    "        print(\"This calculate Influence for data\")\n",
    "        print(\"Provide data as X and target as y\")\n",
    "        print(\"Supported influence methods: LOO, shapley\")\n",
    "        print(\"Supported influence metrics: \")\n",
    "\n",
    "    def Feature_analyze(self, stat=True):\n",
    "        \"\"\"\n",
    "        This extracts each feature column to analyze the influence of each column.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing each features\")\n",
    "\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        n_features = X.shape[1]\n",
    "        if n_features <= 1:\n",
    "            print(\"Data only has one feature.\")\n",
    "            return\n",
    "        # Calculate the base accuracy with all features\n",
    "        model = self.model\n",
    "\n",
    "        self.feature_influences = np.zeros(n_features)\n",
    "\n",
    "        for i in tqdm(range(n_features)):\n",
    "            X_droped = self.X.drop(self.X.columns[i], axis=1)\n",
    "\n",
    "            # fit model\n",
    "            model.fit(X_droped, y)\n",
    "            # calculate the accuracy difference as influence\n",
    "            y_pred = model.predict(X_droped)\n",
    "\n",
    "            current_score = calculate_score_base_on_metric(y_pred, y, self.metric)\n",
    "            influence = calculate_influence_base_on_metric(self.base_score, current_score, self.metric)\n",
    "            self.feature_influences[i] = influence\n",
    "\n",
    "        if stat:\n",
    "            for i in range(n_features):\n",
    "                print(f\"Column: {X.columns[i]}, influence: {self.feature_influences[i]:.4f}\")\n",
    "\n",
    "            min_feature = self.X.iloc[:, [self.feature_influences.argmin()]]\n",
    "\n",
    "            if self.feature_influences.min() >= 0:\n",
    "                print(\"All features have positive impact\")\n",
    "                return\n",
    "\n",
    "            print(f\"The feature has the worst influence: {X.columns[self.feature_influences.argmin()]}, with {self.metric} impact: {self.feature_influences.min()*100:.2f}%\")\n",
    "            print(\"Skewness of the feature:\", skew(min_feature))\n",
    "\n",
    "            sns.histplot(min_feature, kde=True)  # The `kde` parameter adds a Kernel Density Estimate plot over the histogram.\n",
    "            plt.title('Distribution')\n",
    "            plt.xlabel(X.columns[self.feature_influences.argmin()])\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "\n",
    "    def PrintInfluence(self):\n",
    "        if self.influence_method:\n",
    "            print(\"The data last used:\", self.influence_method)\n",
    "            print(self.data_influences)\n",
    "            print(\"Average influence:\", self.data_influences.mean())\n",
    "            print(\"Worst influence:\", self.data_influences.min(), \", index:\", self.data_influences.argmin())\n",
    "            print(\"The data with min influence:\")\n",
    "            print(self.X.iloc[self.data_influences.argmin()])\n",
    "        else:\n",
    "            print(\"No analysis has been done\")\n",
    "\n",
    "    def CalculateInfluence(self, method='LOO', n_random_row=10, num_shuffles=10, threshold=0.97, seed=1, stat=True):\n",
    "        n_random_row = len(self.X) if n_random_row > len(self.X) or n_random_row < 0 else n_random_row\n",
    "        if method == 'LOO':\n",
    "            self.LOOinfluence(n_random_row=n_random_row, seed=seed, stat=stat)\n",
    "        elif method == 'shapley':\n",
    "            self.shapley_influence(num_shuffles=num_shuffles, threshold=threshold, seed=seed, stat=stat)\n",
    "        else:\n",
    "            print(\"Invalid method\")\n",
    "\n",
    "    def LOOinfluence(self, n_random_row, seed=42, stat=True):\n",
    "        # Clear influences\n",
    "        self.data_influences = np.zeros(len(self.X))\n",
    "\n",
    "        print(\"Calculating data influence using Leave One Out\")\n",
    "        # To select 10 random row indexs for LOO\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        selected_indices = np.random.choice(len(self.X), n_random_row, replace=False)\n",
    "\n",
    "        influences = {}\n",
    "        # Calculate the base accuracy with all data points\n",
    "        model = self.model\n",
    "\n",
    "        # Exclue each random row to compute the LOO prediction\n",
    "        for loo_ix in tqdm(selected_indices):\n",
    "            # split data\n",
    "            X_train_loo = self.X.drop(X.index[loo_ix])\n",
    "            y_train_loo = np.delete(self.y, loo_ix)\n",
    "            # fit model\n",
    "            model.fit(X_train_loo, y_train_loo)\n",
    "            # calculate the accuracy difference as influence\n",
    "            y_pred = model.predict(X)\n",
    "            current_score = calculate_score_base_on_metric(y_pred, y, self.metric)\n",
    "            influence = calculate_influence_base_on_metric(self.base_score, current_score, self.metric)\n",
    "\n",
    "            influences[loo_ix] = influence\n",
    "            self.data_influences[loo_ix] = influence\n",
    "\n",
    "        self.influence_method = 'LOO'\n",
    "        if stat:\n",
    "            self.PrintInfluence()\n",
    "\n",
    "        return influences\n",
    "\n",
    "    def shapley_influence(self, num_shuffles=10, threshold=0.97, seed=42, stat=True):\n",
    "        \"\"\"\n",
    "        TMC based shapley inflence calculation\n",
    "        \"\"\"\n",
    "        # Clear influences\n",
    "        self.data_influences = np.zeros(len(self.X))\n",
    "\n",
    "        print(\"Calculating data influence using Shapley Value\")\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        model = self.model\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        full_performance = self.base_score\n",
    "\n",
    "        shapley_values = np.zeros(N)\n",
    "        for j in tqdm(range(num_shuffles)):\n",
    "            permutation = np.random.permutation(N)\n",
    "            X_perm = X.iloc[permutation]\n",
    "            y_perm = y[permutation]\n",
    "\n",
    "            prev_performance = 0\n",
    "            for i in tqdm(range(N)):\n",
    "                model.fit(X_perm[:i+1], y_perm[:i+1])\n",
    "                performance = accuracy_score(y, model.predict(X))\n",
    "                marginal_contribution = performance - prev_performance\n",
    "                shapley_values[permutation[i]] += marginal_contribution\n",
    "                prev_performance = performance\n",
    "\n",
    "                if performance >= threshold * full_performance:\n",
    "                    break\n",
    "\n",
    "        self.data_influences = shapley_values / num_shuffles\n",
    "        self.method = 'shapley'\n",
    "        if stat:\n",
    "            self.PrintInfluence()\n",
    "\n",
    "        return self.data_influences\n",
    "    \n",
    "    def Analyze_data_influence(self, plot=True, negative_threshold=0.15):\n",
    "        if not self.influence_method:\n",
    "            print(\"No data influence computation has been done, returning.\")\n",
    "            return\n",
    "\n",
    "        data_influences = self.data_influences\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "\n",
    "\n",
    "        negative_size = (data_influences < 0).sum()\n",
    "\n",
    "        if negative_size < negative_threshold*X.shape[0]:\n",
    "            print(\"The dataset contains mostly potive data, returning.\")\n",
    "            return\n",
    "\n",
    "        negative_data_points = X[data_influences < 0]\n",
    "        negative_targets = y[data_influences < 0]\n",
    "\n",
    "        features = X.columns\n",
    "        n_features = len(features)\n",
    "\n",
    "        if plot:\n",
    "            n_cols = int(n_features**0.5)\n",
    "            n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 5))  # Adjust size as needed\n",
    "            fig.suptitle('Feature Distributions for Overall and Negative Data Points')\n",
    "\n",
    "            # Flatten the axes array if necessary (for easy indexing)\n",
    "\n",
    "            if n_rows > 1:\n",
    "                axes = axes.flatten()\n",
    "            else:\n",
    "                axes = [axes]\n",
    "\n",
    "            # Loop through the features and plot histograms\n",
    "            for idx, feature in enumerate(features):\n",
    "                # Select the current axis\n",
    "                ax = axes[idx]\n",
    "                # Histogram for the overall dataset\n",
    "                ax.hist(X[feature].dropna(), bins=20, alpha=0.5, label='Overall', color='blue')  # Ensure to drop NA values\n",
    "                # Histogram for the negative data points\n",
    "                ax.hist(negative_data_points[feature].dropna(), bins=20, alpha=0.5, label='Negative', color='red')\n",
    "                ax.set_title(feature)\n",
    "                ax.set_xlabel(feature)\n",
    "                ax.set_ylabel('Frequency')\n",
    "                ax.legend()\n",
    "\n",
    "            # Hide any unused axes if the number of features is odd\n",
    "            if n_features % n_cols != 0:\n",
    "                for ax in axes[n_features:]:\n",
    "                    ax.axis('off')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the rectangle in which to fit the subplots\n",
    "            plt.show()\n",
    "\n",
    "        print(\"Testing distribution different amoung negative data points and original dataset.\")\n",
    "        for feature in features:\n",
    "            # Perform Anderson-Darling test to test if the distribution of that feature is aligned \n",
    "            ks_stat, ks_pvalue = ks_2samp(X[feature].dropna(), negative_data_points[feature].dropna(), method='exact')\n",
    "\n",
    "            # print(f\"Feature: {feature}, KS Statistic: {ks_stat}, P-value: {ks_pvalue}\")\n",
    "\n",
    "            if ks_pvalue < 0.1:\n",
    "                unbalanced_feature = negative_data_points[feature]\n",
    "                print(f\"Feature {feature} distributions is statistically different.\")\n",
    "                print(f\"Consider examine data with feature {feature} with range {unbalanced_feature.mean() - unbalanced_feature.std():.3f} to {unbalanced_feature.mean() + unbalanced_feature.std():.3f}\")\n",
    "\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.hist(X[feature], bins=20, alpha=0.5, label='Overall', color='blue')\n",
    "                plt.hist(negative_data_points[feature], bins=20, alpha=0.5, label='Negative', color='red')\n",
    "                plt.title(f'{feature} Distribution')\n",
    "                plt.xlabel(feature)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "    \n",
    "    def Auto_preprocess(self):\n",
    "        \"\"\"\n",
    "        This auto analyze data points and feature to suggestion an optimal pipeline for dataprocessing\n",
    "        \"\"\"\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        model = self.model\n",
    "        \n",
    "        if not self.feature_influences:\n",
    "            self.Feature_analyze(stat=False)\n",
    "\n",
    "        negative_features = X[X.columns[self.feature_influences < 0]]\n",
    "        \n",
    "        numeric_features = negative_features.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "        categorical_features = negative_features.select_dtypes(include=['object', 'category']).columns\n",
    "        preprocessing_steps = []\n",
    "\n",
    "        current_base_score = self.base_score\n",
    "\n",
    "        # Numeric Feature Preprocessing\n",
    "        for feature in numeric_features:\n",
    "            current_steps = preprocessing_steps\n",
    "            if X[feature].isnull().mean() > 0.1:  # Arbitrary threshold for missing data\n",
    "                preprocessing_steps.append((f'imputer_{feature}', SimpleImputer(strategy='median'), [feature]))\n",
    "                current_base_score = self.try_adding_preprocess(preprocessing_steps, current_base_score, column=feature, method='imputer')\n",
    "                \n",
    "            if X[feature].skew() > 1 or X[feature].skew() < -1:  # Check skewness\n",
    "                preprocessing_steps.append((f'scaler_{feature}', PowerTransformer(method='yeo-johnson'), [feature]))\n",
    "                current_base_score = self.try_adding_preprocess(preprocessing_steps, current_base_score, column=feature, method='scaler')\n",
    "\n",
    "            # None of the method works try removing\n",
    "            if current_steps == preprocessing_steps:\n",
    "                print(f\"None of the preprocess works for this column: {feature}. Consier removing it or examine it\")\n",
    "                \n",
    "\n",
    "        # Categorical Feature Preprocessing\n",
    "        for feature in categorical_features:\n",
    "            current_steps = preprocessing_steps\n",
    "            if X[feature].nunique() > 10:  # Arbitrary cutoff for too many categories\n",
    "                preprocessing_steps.append((f'encoder_{feature}', OneHotEncoder(handle_unknown='ignore'), [feature]))\n",
    "                current_base_score = self.try_adding_preprocess(preprocessing_steps, current_base_score, column=feature, method='encoder')\n",
    "\n",
    "            if X[feature].isnull().mean() > 0.1:\n",
    "                preprocessing_steps.append((f'imputer_{feature}', SimpleImputer(strategy='constant', fill_value='missing'), [feature]))\n",
    "                current_base_score = self.try_adding_preprocess(preprocessing_steps, current_base_score, column=feature, method='imputer')\n",
    "\n",
    "            # None of the method works try removing\n",
    "            if current_steps == preprocessing_steps:\n",
    "                print(f\"None of the preprocess works for this column: {feature}. Consier removing it or examine it\")\n",
    "\n",
    "        # Create the column transformer and pipeline\n",
    "        preprocessor = ColumnTransformer(transformers=preprocessing_steps, remainder='passthrough')\n",
    "        full_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', clone(model))])\n",
    "\n",
    "        # Fit the pipeline\n",
    "        full_pipeline.fit(X, y)\n",
    "\n",
    "        y_pred = full_pipeline.predict(X)\n",
    "        current_score = calculate_score_base_on_metric(y_pred, y, self.metric)\n",
    "\n",
    "        preprocess_influence = -calculate_influence_base_on_metric(self.base_score, current_score, self.metric)\n",
    "\n",
    "        print(f\"Preprocess pipeline: {preprocessing_steps}\")\n",
    "        print(f\"New score {current_score}, with improvement {preprocess_influence}\")\n",
    "\n",
    "        self.preprocess_pipeline = full_pipeline\n",
    "\n",
    "        return full_pipeline\n",
    "\n",
    "    def try_adding_preprocess(self, preprocessing_steps, current_base_score, column, method):\n",
    "        print(f\"Trying {method} on column: {column}\")\n",
    "        preprocessor = ColumnTransformer(transformers=preprocessing_steps, remainder='passthrough')\n",
    "        temp_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', clone(self.model))])\n",
    "        # Fit the pipeline\n",
    "        temp_pipeline.fit(self.X, self.y)\n",
    "        y_pred = temp_pipeline.predict(self.X)\n",
    "        current_score = calculate_score_base_on_metric(y_pred, y, self.metric)\n",
    "        preprocess_influence = -calculate_influence_base_on_metric(current_base_score, current_score, self.metric)\n",
    "        print(f\"This preprocess has influence: {preprocess_influence}\")\n",
    "        if preprocess_influence > 0:\n",
    "            print(\"Performance Improved, saved this preprocess\")\n",
    "            return current_score\n",
    "        else:\n",
    "            print(\"Preprocess dones't work\")\n",
    "            preprocessing_steps.pop()\n",
    "            return current_base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size, X: (2111, 16), y: (2111,)\n",
      "Task: classification, using metric: f1\n",
      "Base score: 0.9990121932919974\n",
      "No data influence computation has been done, returning.\n"
     ]
    }
   ],
   "source": [
    "influenceA = InfluenceAnalyze(random_forest_pipeline, X, y, task=\"classification\", metric=\"f1\")\n",
    "influenceA.Analyze_data_influence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing each features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the preprocess works for this column: Gender. Consier removing it or examine it\n",
      "Trying scaler on column: NCP\n",
      "This preprocess has influence: -1.8044515586956855e-05\n",
      "Preprocess dones't work\n",
      "None of the preprocess works for this column: NCP. Consier removing it or examine it\n",
      "Trying scaler on column: CAEC\n",
      "This preprocess has influence: 0.000987806708002581\n",
      "Performance Improved, saved this preprocess\n",
      "None of the preprocess works for this column: CAEC. Consier removing it or examine it\n",
      "Trying scaler on column: SCC\n",
      "This preprocess has influence: -0.0004951958705641246\n",
      "Preprocess dones't work\n",
      "None of the preprocess works for this column: SCC. Consier removing it or examine it\n",
      "Trying scaler on column: MTRANS\n",
      "This preprocess has influence: -0.0\n",
      "Preprocess dones't work\n",
      "None of the preprocess works for this column: MTRANS. Consier removing it or examine it\n",
      "Preprocess pipeline: [('scaler_CAEC', PowerTransformer(), ['CAEC'])]\n",
      "New score 1.0, with improvement 0.000987806708002581\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;scaler_CAEC&#x27;,\n",
       "                                                  PowerTransformer(),\n",
       "                                                  [&#x27;CAEC&#x27;])])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(max_depth=10, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\" ><label for=\"sk-estimator-id-94\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;scaler_CAEC&#x27;,\n",
       "                                                  PowerTransformer(),\n",
       "                                                  [&#x27;CAEC&#x27;])])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(max_depth=10, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\" ><label for=\"sk-estimator-id-95\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;scaler_CAEC&#x27;, PowerTransformer(), [&#x27;CAEC&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-96\" type=\"checkbox\" ><label for=\"sk-estimator-id-96\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">scaler_CAEC</label><div class=\"sk-toggleable__content\"><pre>[&#x27;CAEC&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-97\" type=\"checkbox\" ><label for=\"sk-estimator-id-97\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-98\" type=\"checkbox\" ><label for=\"sk-estimator-id-98\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Gender&#x27;, &#x27;Age&#x27;, &#x27;Height&#x27;, &#x27;Weight&#x27;, &#x27;family_history_with_overweight&#x27;, &#x27;FAVC&#x27;, &#x27;FCVC&#x27;, &#x27;NCP&#x27;, &#x27;SMOKE&#x27;, &#x27;CH2O&#x27;, &#x27;SCC&#x27;, &#x27;FAF&#x27;, &#x27;TUE&#x27;, &#x27;CALC&#x27;, &#x27;MTRANS&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\" ><label for=\"sk-estimator-id-99\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\" ><label for=\"sk-estimator-id-100\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('scaler_CAEC',\n",
       "                                                  PowerTransformer(),\n",
       "                                                  ['CAEC'])])),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(max_depth=10, random_state=42))])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influenceA.Auto_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyzer\n",
    "\n",
    "A Python library for analyzing the influence of individual data points and features on machine learning models. Currently supporting Sklearn models, with plans to extend support to PyTorch models.\n",
    "\n",
    "## Installation\n",
    "Install Data Analyzer using pip:\n",
    "\n",
    "```bash\n",
    "pip install data_analyze_tool\n",
    "```\n",
    "\n",
    "```python\n",
    "from data_analyze_tool import AnalyzeModel\n",
    "\n",
    "# Initialize the model analysis with your model and dataset\n",
    "myInfluenceModel = AnalyzeModel(my_model, X, y, task=\"classification\", metric=\"f1\")\n",
    "\n",
    "# Analyze the influence of data\n",
    "myInfluenceModel.analyze_data_influence()\n",
    "\n",
    "# Automatically preprocess data\n",
    "myInfluenceModel.auto_preprocess()\n",
    "```\n",
    "\n",
    "Provide the model with your model and data, the model can do analysis in both direction X (individual data influence) by Leave One Out or Shapley Value and direction Y (features by statistic analysis).\n",
    "\n",
    "The model can auto analyze the features of the model and can provide a pipeline about how to preprocess the model.\n",
    "\n",
    "\n",
    "## Demo\n",
    "\n",
    "```bash\n",
    "Analyzing each features\n",
    "100%|██████████| 16/16 [00:03<00:00,  4.45it/s]\n",
    "None of the preprocess works for this column: Gender. Consier removing it or examine it\n",
    "Trying scaler on column: NCP\n",
    "This preprocess has influence: -1.8044515586956855e-05\n",
    "Preprocess dones't work\n",
    "None of the preprocess works for this column: NCP. Consier removing it or examine it\n",
    "Trying scaler on column: CAEC\n",
    "This preprocess has influence: 0.000987806708002581\n",
    "Performance Improved, saved this preprocess\n",
    "None of the preprocess works for this column: CAEC. Consier removing it or examine it\n",
    "Trying scaler on column: SCC\n",
    "This preprocess has influence: -0.0004951958705641246\n",
    "Preprocess dones't work\n",
    "None of the preprocess works for this column: SCC. Consier removing it or examine it\n",
    "Trying scaler on column: MTRANS\n",
    "This preprocess has influence: -0.0\n",
    "Preprocess dones't work\n",
    "None of the preprocess works for this column: MTRANS. Consier removing it or examine it\n",
    "Preprocess pipeline: [('scaler_CAEC', PowerTransformer(), ['CAEC'])]\n",
    "New score 1.0, with improvement 0.000987806708002581\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
